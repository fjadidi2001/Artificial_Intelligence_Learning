{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOyKnHv880BDrL1VUavbOJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Artificial_Intelligence_Learning/blob/master/Copy_of_Copy_of_Copy_of_telematics_syn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('telematics_syn.csv')\n",
        "\n",
        "# 1. Handling Missing Values\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values in each column:\\n\", missing_values)\n",
        "\n",
        "# Impute missing values (using median for numerical and most frequent for categorical)\n",
        "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "# Imputation transformers\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine transformers into a single preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Apply the preprocessing steps\n",
        "df_preprocessed = preprocessor.fit_transform(df)\n",
        "\n",
        "# Convert the result back to a DataFrame\n",
        "df_preprocessed = pd.DataFrame(df_preprocessed, columns=numerical_features + list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)))\n",
        "\n",
        "# Check the shape of the preprocessed data\n",
        "print(\"Shape of preprocessed data:\", df_preprocessed.shape)\n",
        "\n",
        "# Save the preprocessed DataFrame to a new CSV file\n",
        "df_preprocessed.to_csv('telematics_syn_preprocessed.csv', index=False)\n",
        "\n",
        "print(\"Preprocessing completed and saved to 'telematics_syn_preprocessed.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMyAAm9K5ynk",
        "outputId": "0f521e7d-8506-4b66-83b8-00e878f1ee70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            " Duration                  0\n",
            "Insured.age               0\n",
            "Insured.sex               0\n",
            "Car.age                   0\n",
            "Marital                   0\n",
            "Car.use                   0\n",
            "Credit.score              0\n",
            "Region                    0\n",
            "Annual.miles.drive        0\n",
            "Years.noclaims            0\n",
            "Territory                 0\n",
            "Annual.pct.driven         0\n",
            "Total.miles.driven        0\n",
            "Pct.drive.mon             0\n",
            "Pct.drive.tue             0\n",
            "Pct.drive.wed             0\n",
            "Pct.drive.thr             0\n",
            "Pct.drive.fri             0\n",
            "Pct.drive.sat             0\n",
            "Pct.drive.sun             0\n",
            "Pct.drive.2hrs            0\n",
            "Pct.drive.3hrs            0\n",
            "Pct.drive.4hrs            0\n",
            "Pct.drive.wkday           0\n",
            "Pct.drive.wkend           0\n",
            "Pct.drive.rush am         0\n",
            "Pct.drive.rush pm         0\n",
            "Avgdays.week              0\n",
            "Accel.06miles             0\n",
            "Accel.08miles             0\n",
            "Accel.09miles             0\n",
            "Accel.11miles             0\n",
            "Accel.12miles             0\n",
            "Accel.14miles             0\n",
            "Brake.06miles             0\n",
            "Brake.08miles             0\n",
            "Brake.09miles             0\n",
            "Brake.11miles             0\n",
            "Brake.12miles             0\n",
            "Brake.14miles             0\n",
            "Left.turn.intensity08     0\n",
            "Left.turn.intensity09     0\n",
            "Left.turn.intensity10     0\n",
            "Left.turn.intensity11     0\n",
            "Left.turn.intensity12     0\n",
            "Right.turn.intensity08    0\n",
            "Right.turn.intensity09    0\n",
            "Right.turn.intensity10    0\n",
            "Right.turn.intensity11    0\n",
            "Right.turn.intensity12    0\n",
            "NB_Claim                  0\n",
            "AMT_Claim                 0\n",
            "dtype: int64\n",
            "Shape of preprocessed data: (100000, 58)\n",
            "Preprocessing completed and saved to 'telematics_syn_preprocessed.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "df_preprocessed = pd.read_csv('telematics_syn_preprocessed.csv')\n",
        "\n",
        "# Split the dataset into train (70%), validation (15%), and test (15%) sets\n",
        "train_size = 0.7\n",
        "val_size = 0.15\n",
        "test_size = 0.15\n",
        "\n",
        "# Split the data into train and temp sets (train + validation + test)\n",
        "train_df, temp_df = train_test_split(df_preprocessed, train_size=train_size, random_state=42)\n",
        "\n",
        "# Split the temp set into validation and test sets\n",
        "val_df, test_df = train_test_split(temp_df, test_size=test_size/(val_size + test_size), random_state=42)\n",
        "\n",
        "# Print the sizes of each set\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "# Save the splits to CSV files (optional)\n",
        "train_df.to_csv('telematics_train.csv', index=False)\n",
        "val_df.to_csv('telematics_val.csv', index=False)\n",
        "test_df.to_csv('telematics_test.csv', index=False)\n",
        "\n",
        "print(\"Dataset splits saved to 'telematics_train.csv', 'telematics_val.csv', and 'telematics_test.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_Xx1N1e6WSU",
        "outputId": "29efa3d7-c9d6-44d9-bc3c-60071f061911"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 70000\n",
            "Validation set size: 15000\n",
            "Test set size: 15000\n",
            "Dataset splits saved to 'telematics_train.csv', 'telematics_val.csv', and 'telematics_test.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJV7TWBk7Jfs",
        "outputId": "6dc4266c-f8af-42ee-fd28-9fa7ee29407f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.25.2)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.2.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.66.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-tabnet\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pytorch-tabnet-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the preprocessed dataset splits\n",
        "train_df = pd.read_csv('telematics_train.csv')\n",
        "val_df = pd.read_csv('telematics_val.csv')\n",
        "test_df = pd.read_csv('telematics_test.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X_train = train_df.drop(columns=['NB_Claim'])  # Replace 'NB_Claim' with your target column name\n",
        "y_train = train_df['NB_Claim'].astype(int)  # Convert target to integer labels\n",
        "\n",
        "X_val = val_df.drop(columns=['NB_Claim'])\n",
        "y_val = val_df['NB_Claim'].astype(int)  # Convert target to integer labels\n",
        "\n",
        "X_test = test_df.drop(columns=['NB_Claim'])\n",
        "y_test = test_df['NB_Claim'].astype(int)  # Convert target to integer labels\n",
        "\n",
        "# Initialize the TabNet model\n",
        "tabnet_model = TabNetClassifier()\n",
        "\n",
        "# Train the TabNet model\n",
        "tabnet_model.fit(\n",
        "    X_train=X_train.values, y_train=y_train.values,\n",
        "    eval_set=[(X_val.values, y_val.values)],\n",
        "    eval_name=['val'],\n",
        "    eval_metric=['accuracy'],\n",
        "    max_epochs=1000, patience=50,\n",
        "    batch_size=1024, virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    weights=1,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = tabnet_model.predict(X_test.values)\n",
        "test_accuracy = accuracy_score(y_test.values, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test.values, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test.values, y_pred))\n",
        "\n",
        "# Save the model\n",
        "tabnet_model.save_model('tabnet_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nmMsN-S7z0l",
        "outputId": "f850cd2e-41f1-4162-c720-9123b046e0e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.94992 | val_accuracy: 0.5086  |  0:00:06s\n",
            "epoch 1  | loss: 0.46054 | val_accuracy: 0.03853 |  0:00:11s\n",
            "epoch 2  | loss: 0.34196 | val_accuracy: 0.035   |  0:00:16s\n",
            "epoch 3  | loss: 0.28742 | val_accuracy: 0.20013 |  0:00:22s\n",
            "epoch 4  | loss: 0.2458  | val_accuracy: 0.15913 |  0:00:27s\n",
            "epoch 5  | loss: 0.21692 | val_accuracy: 0.96667 |  0:00:32s\n",
            "epoch 6  | loss: 0.17786 | val_accuracy: 0.05    |  0:00:38s\n",
            "epoch 7  | loss: 0.1902  | val_accuracy: 0.04827 |  0:00:43s\n",
            "epoch 8  | loss: 0.16078 | val_accuracy: 0.98247 |  0:00:48s\n",
            "epoch 9  | loss: 0.16651 | val_accuracy: 0.9848  |  0:00:54s\n",
            "epoch 10 | loss: 0.13779 | val_accuracy: 0.09567 |  0:00:58s\n",
            "epoch 11 | loss: 0.12122 | val_accuracy: 0.9842  |  0:01:04s\n",
            "epoch 12 | loss: 0.1205  | val_accuracy: 0.04673 |  0:01:10s\n",
            "epoch 13 | loss: 0.11444 | val_accuracy: 0.06353 |  0:01:14s\n",
            "epoch 14 | loss: 0.09627 | val_accuracy: 0.6186  |  0:01:21s\n",
            "epoch 15 | loss: 0.11322 | val_accuracy: 0.05473 |  0:01:25s\n",
            "epoch 16 | loss: 0.09425 | val_accuracy: 0.05993 |  0:01:30s\n",
            "epoch 17 | loss: 0.11187 | val_accuracy: 0.04827 |  0:01:37s\n",
            "epoch 18 | loss: 0.09897 | val_accuracy: 0.06753 |  0:01:41s\n",
            "epoch 19 | loss: 0.11022 | val_accuracy: 0.2238  |  0:01:46s\n",
            "epoch 20 | loss: 0.09876 | val_accuracy: 0.05107 |  0:01:52s\n",
            "epoch 21 | loss: 0.09053 | val_accuracy: 0.0468  |  0:01:57s\n",
            "epoch 22 | loss: 0.08613 | val_accuracy: 0.04513 |  0:02:03s\n",
            "epoch 23 | loss: 0.0875  | val_accuracy: 0.04793 |  0:02:08s\n",
            "epoch 24 | loss: 0.0751  | val_accuracy: 0.0472  |  0:02:13s\n",
            "epoch 25 | loss: 0.0925  | val_accuracy: 0.0574  |  0:02:19s\n",
            "epoch 26 | loss: 0.11631 | val_accuracy: 0.04347 |  0:02:24s\n",
            "epoch 27 | loss: 0.08675 | val_accuracy: 0.27593 |  0:02:29s\n",
            "epoch 28 | loss: 0.08324 | val_accuracy: 0.08787 |  0:02:35s\n",
            "epoch 29 | loss: 0.0787  | val_accuracy: 0.04273 |  0:02:40s\n",
            "epoch 30 | loss: 0.0735  | val_accuracy: 0.04267 |  0:02:45s\n",
            "epoch 31 | loss: 0.07645 | val_accuracy: 0.03927 |  0:02:51s\n",
            "epoch 32 | loss: 0.06869 | val_accuracy: 0.03993 |  0:02:56s\n",
            "epoch 33 | loss: 0.06949 | val_accuracy: 0.04053 |  0:03:01s\n",
            "epoch 34 | loss: 0.07159 | val_accuracy: 0.04033 |  0:03:07s\n",
            "epoch 35 | loss: 0.08936 | val_accuracy: 0.04233 |  0:03:11s\n",
            "epoch 36 | loss: 0.0915  | val_accuracy: 0.03993 |  0:03:17s\n",
            "epoch 37 | loss: 0.08022 | val_accuracy: 0.04007 |  0:03:23s\n",
            "epoch 38 | loss: 0.08982 | val_accuracy: 0.04373 |  0:03:27s\n",
            "epoch 39 | loss: 0.08103 | val_accuracy: 0.03967 |  0:03:34s\n",
            "epoch 40 | loss: 0.07299 | val_accuracy: 0.03973 |  0:03:38s\n",
            "epoch 41 | loss: 0.07618 | val_accuracy: 0.03987 |  0:03:43s\n",
            "epoch 42 | loss: 0.07404 | val_accuracy: 0.03967 |  0:03:49s\n",
            "epoch 43 | loss: 0.05841 | val_accuracy: 0.03953 |  0:03:54s\n",
            "epoch 44 | loss: 0.06111 | val_accuracy: 0.03973 |  0:03:59s\n",
            "epoch 45 | loss: 0.07466 | val_accuracy: 0.03933 |  0:04:05s\n",
            "epoch 46 | loss: 0.06453 | val_accuracy: 0.0396  |  0:04:10s\n",
            "epoch 47 | loss: 0.05834 | val_accuracy: 0.03967 |  0:04:15s\n",
            "epoch 48 | loss: 0.0642  | val_accuracy: 0.0396  |  0:04:23s\n",
            "epoch 49 | loss: 0.06015 | val_accuracy: 0.03987 |  0:04:28s\n",
            "epoch 50 | loss: 0.05368 | val_accuracy: 0.91093 |  0:04:34s\n",
            "epoch 51 | loss: 0.05656 | val_accuracy: 0.0396  |  0:04:39s\n",
            "epoch 52 | loss: 0.05757 | val_accuracy: 0.03973 |  0:04:44s\n",
            "epoch 53 | loss: 0.06055 | val_accuracy: 0.0398  |  0:04:50s\n",
            "epoch 54 | loss: 0.05749 | val_accuracy: 0.03947 |  0:04:55s\n",
            "epoch 55 | loss: 0.05018 | val_accuracy: 0.03967 |  0:04:59s\n",
            "epoch 56 | loss: 0.05065 | val_accuracy: 0.03953 |  0:05:06s\n",
            "epoch 57 | loss: 0.04931 | val_accuracy: 0.03967 |  0:05:10s\n",
            "epoch 58 | loss: 0.04656 | val_accuracy: 0.0398  |  0:05:16s\n",
            "epoch 59 | loss: 0.05303 | val_accuracy: 0.04013 |  0:05:22s\n",
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 9 and best_val_accuracy = 0.9848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9828666666666667\n",
            "Confusion Matrix:\n",
            "[[14200   109     3     3]\n",
            " [   82   518    52     0]\n",
            " [    2     6    22     0]\n",
            " [    0     0     0     3]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     14315\n",
            "           4       0.82      0.79      0.81       652\n",
            "           8       0.29      0.73      0.41        30\n",
            "          13       0.50      1.00      0.67         3\n",
            "\n",
            "    accuracy                           0.98     15000\n",
            "   macro avg       0.65      0.88      0.72     15000\n",
            "weighted avg       0.98      0.98      0.98     15000\n",
            "\n",
            "Successfully saved model at tabnet_model.zip\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tabnet_model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the preprocessed dataset splits\n",
        "train_df = pd.read_csv('telematics_train.csv')\n",
        "val_df = pd.read_csv('telematics_val.csv')\n",
        "test_df = pd.read_csv('telematics_test.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X_train = train_df.drop(columns=['NB_Claim'])  # Replace 'NB_Claim' with your target column name\n",
        "y_train = train_df['NB_Claim']\n",
        "\n",
        "X_val = val_df.drop(columns=['NB_Claim'])\n",
        "y_val = val_df['NB_Claim']\n",
        "\n",
        "X_test = test_df.drop(columns=['NB_Claim'])\n",
        "y_test = test_df['NB_Claim']\n",
        "\n",
        "# Convert target variables to integer labels (assuming it's a classification problem)\n",
        "# Adjust the threshold and logic based on your problem definition\n",
        "threshold = 0  # Example threshold for binary classification\n",
        "y_train = (y_train > threshold).astype(int)\n",
        "y_val = (y_val > threshold).astype(int)\n",
        "y_test = (y_test > threshold).astype(int)\n",
        "\n",
        "# Initialize the TabNet model\n",
        "tabnet_model = TabNetClassifier()\n",
        "\n",
        "# Train the TabNet model\n",
        "history = tabnet_model.fit(\n",
        "    X_train=X_train.values, y_train=y_train.values,\n",
        "    eval_set=[(X_train.values, y_train.values), (X_val.values, y_val.values)],\n",
        "    eval_name=['train', 'val'],\n",
        "    eval_metric=['accuracy'],\n",
        "    max_epochs=1000, patience=50,\n",
        "    batch_size=1024, virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    weights=1,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = tabnet_model.predict(X_test.values)\n",
        "test_accuracy = accuracy_score(y_test.values, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test.values, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test.values, y_pred))\n",
        "\n",
        "# Save the model\n",
        "tabnet_model.save_model('tabnet_model')\n",
        "\n",
        "# Extract history for plotting\n",
        "train_losses = history.history['loss']\n",
        "train_accuracies = history.history['train_accuracy']\n",
        "val_accuracies = history.history['val_accuracy']\n",
        "\n",
        "# Plot accuracy over epochs\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_accuracies, label='Training Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training loss over epochs\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FzQElSCN9eh6",
        "outputId": "7b9eea9f-3179-4960-99ec-f6414f4458d1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.4831  | train_accuracy: 0.11404 | val_accuracy: 0.10907 |  0:00:08s\n",
            "epoch 1  | loss: 0.21842 | train_accuracy: 0.04853 | val_accuracy: 0.0464  |  0:00:16s\n",
            "epoch 2  | loss: 0.18378 | train_accuracy: 0.0462  | val_accuracy: 0.04453 |  0:00:24s\n",
            "epoch 3  | loss: 0.16759 | train_accuracy: 0.04841 | val_accuracy: 0.04733 |  0:00:32s\n",
            "epoch 4  | loss: 0.16409 | train_accuracy: 0.04726 | val_accuracy: 0.04613 |  0:00:40s\n",
            "epoch 5  | loss: 0.15097 | train_accuracy: 0.04386 | val_accuracy: 0.04267 |  0:00:49s\n",
            "epoch 6  | loss: 0.15326 | train_accuracy: 0.0938  | val_accuracy: 0.09213 |  0:00:56s\n",
            "epoch 7  | loss: 0.15468 | train_accuracy: 0.04481 | val_accuracy: 0.04307 |  0:01:05s\n",
            "epoch 8  | loss: 0.15368 | train_accuracy: 0.04316 | val_accuracy: 0.0412  |  0:01:12s\n",
            "epoch 9  | loss: 0.1421  | train_accuracy: 0.04313 | val_accuracy: 0.04133 |  0:01:20s\n",
            "epoch 10 | loss: 0.13718 | train_accuracy: 0.04287 | val_accuracy: 0.04093 |  0:01:27s\n",
            "epoch 11 | loss: 0.13756 | train_accuracy: 0.04251 | val_accuracy: 0.04073 |  0:01:36s\n",
            "epoch 12 | loss: 0.13769 | train_accuracy: 0.04337 | val_accuracy: 0.0418  |  0:01:43s\n",
            "epoch 13 | loss: 0.13207 | train_accuracy: 0.0437  | val_accuracy: 0.04233 |  0:01:52s\n",
            "epoch 14 | loss: 0.13192 | train_accuracy: 0.0434  | val_accuracy: 0.04187 |  0:01:59s\n",
            "epoch 15 | loss: 0.12721 | train_accuracy: 0.04359 | val_accuracy: 0.04227 |  0:02:08s\n",
            "epoch 16 | loss: 0.12421 | train_accuracy: 0.04317 | val_accuracy: 0.0412  |  0:02:15s\n",
            "epoch 17 | loss: 0.11771 | train_accuracy: 0.04326 | val_accuracy: 0.0414  |  0:02:23s\n",
            "epoch 18 | loss: 0.11252 | train_accuracy: 0.04359 | val_accuracy: 0.04187 |  0:02:31s\n",
            "epoch 19 | loss: 0.11356 | train_accuracy: 0.0429  | val_accuracy: 0.04107 |  0:02:39s\n",
            "epoch 20 | loss: 0.10446 | train_accuracy: 0.04297 | val_accuracy: 0.04127 |  0:02:48s\n",
            "epoch 21 | loss: 0.10331 | train_accuracy: 0.04256 | val_accuracy: 0.04073 |  0:02:55s\n",
            "epoch 22 | loss: 0.09776 | train_accuracy: 0.04277 | val_accuracy: 0.04093 |  0:03:03s\n",
            "epoch 23 | loss: 0.09488 | train_accuracy: 0.04253 | val_accuracy: 0.04073 |  0:03:10s\n",
            "epoch 24 | loss: 0.09145 | train_accuracy: 0.04299 | val_accuracy: 0.04133 |  0:03:19s\n",
            "epoch 25 | loss: 0.08471 | train_accuracy: 0.04251 | val_accuracy: 0.04073 |  0:03:26s\n",
            "epoch 26 | loss: 0.08119 | train_accuracy: 0.04251 | val_accuracy: 0.04073 |  0:03:35s\n",
            "epoch 27 | loss: 0.07795 | train_accuracy: 0.04251 | val_accuracy: 0.04073 |  0:03:42s\n",
            "epoch 28 | loss: 0.07478 | train_accuracy: 0.04256 | val_accuracy: 0.04073 |  0:03:51s\n",
            "epoch 29 | loss: 0.07445 | train_accuracy: 0.04251 | val_accuracy: 0.04073 |  0:03:58s\n",
            "epoch 30 | loss: 0.06796 | train_accuracy: 0.04251 | val_accuracy: 0.04073 |  0:04:07s\n",
            "epoch 31 | loss: 0.06634 | train_accuracy: 0.04266 | val_accuracy: 0.041   |  0:04:14s\n",
            "epoch 32 | loss: 0.06287 | train_accuracy: 0.04266 | val_accuracy: 0.04093 |  0:04:22s\n",
            "epoch 33 | loss: 0.05916 | train_accuracy: 0.0427  | val_accuracy: 0.04113 |  0:04:31s\n",
            "epoch 34 | loss: 0.05638 | train_accuracy: 0.04261 | val_accuracy: 0.04087 |  0:04:38s\n",
            "epoch 35 | loss: 0.05687 | train_accuracy: 0.04259 | val_accuracy: 0.0408  |  0:04:47s\n",
            "epoch 36 | loss: 0.05331 | train_accuracy: 0.04251 | val_accuracy: 0.04073 |  0:04:56s\n",
            "epoch 37 | loss: 0.05193 | train_accuracy: 0.04253 | val_accuracy: 0.04073 |  0:05:07s\n",
            "epoch 38 | loss: 0.06458 | train_accuracy: 0.04257 | val_accuracy: 0.04073 |  0:05:14s\n",
            "epoch 39 | loss: 0.05112 | train_accuracy: 0.04297 | val_accuracy: 0.0416  |  0:05:22s\n",
            "epoch 40 | loss: 0.04516 | train_accuracy: 0.0426  | val_accuracy: 0.04107 |  0:05:30s\n",
            "epoch 41 | loss: 0.04761 | train_accuracy: 0.04294 | val_accuracy: 0.041   |  0:05:38s\n",
            "epoch 42 | loss: 0.04439 | train_accuracy: 0.04251 | val_accuracy: 0.04073 |  0:05:47s\n",
            "epoch 43 | loss: 0.04544 | train_accuracy: 0.04253 | val_accuracy: 0.04073 |  0:05:54s\n",
            "epoch 44 | loss: 0.03862 | train_accuracy: 0.04251 | val_accuracy: 0.04073 |  0:06:03s\n",
            "epoch 45 | loss: 0.04339 | train_accuracy: 0.04253 | val_accuracy: 0.04073 |  0:06:10s\n",
            "epoch 46 | loss: 0.04044 | train_accuracy: 0.04253 | val_accuracy: 0.04073 |  0:06:19s\n",
            "epoch 47 | loss: 0.03767 | train_accuracy: 0.04256 | val_accuracy: 0.04087 |  0:06:26s\n",
            "epoch 48 | loss: 0.04284 | train_accuracy: 0.04256 | val_accuracy: 0.04087 |  0:06:34s\n",
            "epoch 49 | loss: 0.03802 | train_accuracy: 0.04294 | val_accuracy: 0.0414  |  0:06:42s\n",
            "epoch 50 | loss: 0.03794 | train_accuracy: 0.04277 | val_accuracy: 0.04133 |  0:06:50s\n",
            "\n",
            "Early stopping occurred at epoch 50 with best_epoch = 0 and best_val_accuracy = 0.10907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.112\n",
            "Confusion Matrix:\n",
            "[[ 1016 13299]\n",
            " [   21   664]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.07      0.13     14315\n",
            "           1       0.05      0.97      0.09       685\n",
            "\n",
            "    accuracy                           0.11     15000\n",
            "   macro avg       0.51      0.52      0.11     15000\n",
            "weighted avg       0.94      0.11      0.13     15000\n",
            "\n",
            "Successfully saved model at tabnet_model.zip\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'history'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-71ba4a281a1b>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Extract history for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mtrain_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mval_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'history'"
          ]
        }
      ]
    }
  ]
}