{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6eAR91ScQrw0xc7eotiGx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Artificial_Intelligence_Learning/blob/master/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting with Google Drive Files and Folders"
      ],
      "metadata": {
        "id": "18k8UnSm63fV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAR5VmbI4tT1",
        "outputId": "f4a7f3d0-d89d-42ae-8192-d436615afe1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mounting the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_root='/content/drive/My Drive/Deep learning/ChatBot'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Relevant Libraries"
      ],
      "metadata": {
        "id": "MP1AzSMN6881"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZc9QFCq60ya",
        "outputId": "c0430242-0001-49e7-c296-9aa737868d6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the JSON file"
      ],
      "metadata": {
        "id": "Q4agKX5A81V9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = open(data_root+'/intents.json').read()\n",
        "data = json.loads(data_file)"
      ],
      "metadata": {
        "id": "vCF8hSVw75yk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identifying Feature and Target for the NLP Model\n",
        "\n",
        "- The words have been stored in data_X and the corresponding tag to it has been stored in data_Y.\n",
        "- Two more lists: words and classes containing all tokens and corresponding tags have also been created.\n",
        "- For the list words, the punctuations have not been added by using a simple conditional statement and the words have been converted into their root words using NLTK's WordNetLemmatizer().\n",
        "-  At last, both the lists have been sorted and these functions have been used to remove any duplicates.\n"
      ],
      "metadata": {
        "id": "I3JciFFH_Cyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating data_x and data_y\n",
        "words = [] # vocab for pattern\n",
        "classes = [] # vocab for tags\n",
        "data_x = [] # for storing each pattern\n",
        "data_y = [] # for storing tag corresponding to each pattern in data_x\n",
        "\n",
        "# iterate overall the intents\n",
        "for intent in data[\"intents\"]:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "    tokens = nltk.word_tokenize(pattern) # tokenize each pattern\n",
        "    words.extend(tokens) # append tokens to word\n",
        "    data_x.append(pattern) # appending pattern to data_x\n",
        "    data_y.append(intent[\"tag\"]) # appending the associated tag to each pattern\n",
        "\n",
        "\n",
        "    # adding the tag to the classes if it's not there alredy\n",
        "    if intent[\"tag\"] not in classes:\n",
        "      classes.append(intent[\"tag\"])\n",
        "# init lemmatizer to get stem of the words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
        "# sorte alphabetical and sure no duplicated\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))"
      ],
      "metadata": {
        "id": "rde8i8hq9Umb"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}