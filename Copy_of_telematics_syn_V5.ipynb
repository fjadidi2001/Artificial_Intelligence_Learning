{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Artificial_Intelligence_Learning/blob/master/Copy_of_telematics_syn_V5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbHLksJUkNON",
        "outputId": "889ee3f5-a2e1-400c-8bfd-10bda2dceec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "(100000, 52)\n",
            "   Duration  Insured.age Insured.sex  Car.age  Marital  Car.use  Credit.score  \\\n",
            "0       366           45        Male       -1  Married  Commute         609.0   \n",
            "1       182           44      Female        3  Married  Commute         575.0   \n",
            "2       184           48      Female        6  Married  Commute         847.0   \n",
            "3       183           71        Male        6  Married  Private         842.0   \n",
            "4       183           84        Male       10  Married  Private         856.0   \n",
            "\n",
            "  Region  Annual.miles.drive  Years.noclaims  ...  Left.turn.intensity10  \\\n",
            "0  Urban             6213.71              25  ...                    1.0   \n",
            "1  Urban            12427.42              20  ...                   58.0   \n",
            "2  Urban            12427.42              14  ...                    0.0   \n",
            "3  Urban             6213.71              43  ...                    0.0   \n",
            "4  Urban             6213.71              65  ...                    2.0   \n",
            "\n",
            "   Left.turn.intensity11  Left.turn.intensity12  Right.turn.intensity08  \\\n",
            "0                    0.0                    0.0                     3.0   \n",
            "1                   24.0                   11.0                  1099.0   \n",
            "2                    0.0                    0.0                     0.0   \n",
            "3                    0.0                    0.0                     0.0   \n",
            "4                    0.0                    0.0                   325.0   \n",
            "\n",
            "   Right.turn.intensity09  Right.turn.intensity10  Right.turn.intensity11  \\\n",
            "0                     1.0                     0.0                     0.0   \n",
            "1                   615.0                   219.0                   101.0   \n",
            "2                     0.0                     0.0                     0.0   \n",
            "3                     0.0                     0.0                     0.0   \n",
            "4                   111.0                    18.0                     4.0   \n",
            "\n",
            "   Right.turn.intensity12  NB_Claim    AMT_Claim  \n",
            "0                     0.0         1  5100.171753  \n",
            "1                    40.0         1   883.554840  \n",
            "2                     0.0         0     0.000000  \n",
            "3                     0.0         0     0.000000  \n",
            "4                     2.0         0     0.000000  \n",
            "\n",
            "[5 rows x 52 columns]\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify file path\n",
        "file_path = '/content/drive/My Drive/telematics_syn.csv'\n",
        "\n",
        "# Import pandas (assuming you want to use it to read the CSV)\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.shape)  # Should print (100000, 52)\n",
        "print(df.head()) # To check the first few rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pre-process"
      ],
      "metadata": {
        "id": "mphEoc3iOfOr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UNDVweYpQFO",
        "outputId": "9e2ad52a-4f8c-45b2-dff2-38006b463456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "(100000, 52)\n",
            "Index(['Duration', 'Insured.age', 'Insured.sex', 'Car.age', 'Marital',\n",
            "       'Car.use', 'Credit.score', 'Region', 'Annual.miles.drive',\n",
            "       'Years.noclaims', 'Territory', 'Annual.pct.driven',\n",
            "       'Total.miles.driven', 'Pct.drive.mon', 'Pct.drive.tue', 'Pct.drive.wed',\n",
            "       'Pct.drive.thr', 'Pct.drive.fri', 'Pct.drive.sat', 'Pct.drive.sun',\n",
            "       'Pct.drive.2hrs', 'Pct.drive.3hrs', 'Pct.drive.4hrs', 'Pct.drive.wkday',\n",
            "       'Pct.drive.wkend', 'Pct.drive.rush am', 'Pct.drive.rush pm',\n",
            "       'Avgdays.week', 'Accel.06miles', 'Accel.08miles', 'Accel.09miles',\n",
            "       'Accel.11miles', 'Accel.12miles', 'Accel.14miles', 'Brake.06miles',\n",
            "       'Brake.08miles', 'Brake.09miles', 'Brake.11miles', 'Brake.12miles',\n",
            "       'Brake.14miles', 'Left.turn.intensity08', 'Left.turn.intensity09',\n",
            "       'Left.turn.intensity10', 'Left.turn.intensity11',\n",
            "       'Left.turn.intensity12', 'Right.turn.intensity08',\n",
            "       'Right.turn.intensity09', 'Right.turn.intensity10',\n",
            "       'Right.turn.intensity11', 'Right.turn.intensity12', 'NB_Claim',\n",
            "       'AMT_Claim'],\n",
            "      dtype='object')\n",
            "(100000, 113)\n",
            "(100000, 113)\n",
            "(100000, 114)\n",
            "(64000, 113) (16000, 113) (20000, 113)\n",
            "(64000, 113) (16000, 113) (20000, 113)\n",
            "(64000, 114) (16000, 114) (20000, 114)\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Specify file path\n",
        "file_path = '/content/drive/My Drive/telematics_syn.csv'\n",
        "\n",
        "# Import pandas (assuming you want to use it to read the CSV)\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.shape)  # Should print (100000, 52)\n",
        "print(df.columns) # To check the column names\n",
        "\n",
        "# Encoding categorical columns using one-hot encoding\n",
        "categorical_cols = ['Marital', 'Insured.sex', 'Car.use', 'Region', 'Territory']\n",
        "df_level1 = pd.get_dummies(df, columns=categorical_cols)\n",
        "\n",
        "# Creating the ClaimYN variable\n",
        "df_level1['ClaimYN'] = df_level1['NB_Claim'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Save the preprocessed data to a new file\n",
        "preprocessed_file_path_level1 = '/content/drive/My Drive/pre_telematics_syn_level1.csv'\n",
        "df_level1.to_csv(preprocessed_file_path_level1, index=False)\n",
        "print(df_level1.shape)  # Should print (100000, number of columns after encoding)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Copy the DataFrame from Level 1\n",
        "df_level2 = df_level1.copy()\n",
        "\n",
        "# Feature columns (excluding response columns)\n",
        "feature_cols = [col for col in df_level2.columns if col not in ['NB_Claim', 'AMT_Claim', 'ClaimYN']]\n",
        "\n",
        "# Applying StandardScaler and MinMaxScaler\n",
        "scaler = StandardScaler()\n",
        "minmax_scaler = MinMaxScaler()\n",
        "\n",
        "df_level2[feature_cols] = scaler.fit_transform(df_level2[feature_cols])\n",
        "df_level2[feature_cols] = minmax_scaler.fit_transform(df_level2[feature_cols])\n",
        "\n",
        "# Save the preprocessed data to a new file\n",
        "preprocessed_file_path_level2 = '/content/drive/My Drive/pre_telematics_syn_level2.csv'\n",
        "df_level2.to_csv(preprocessed_file_path_level2, index=False)\n",
        "print(df_level2.shape)  # Should print (100000, number of columns after scaling and normalization)\n",
        "\n",
        "# Copy the DataFrame from Level 2\n",
        "df_level3 = df_level2.copy()\n",
        "\n",
        "# Use provided columns for feature engineering\n",
        "feature_cols_to_sum = ['Annual.pct.driven', 'Annual.miles.drive', 'Pct.drive.rush am']\n",
        "\n",
        "# Advanced feature engineering steps\n",
        "df_level3['DrivingIntensity'] = df_level3[feature_cols_to_sum].sum(axis=1)\n",
        "\n",
        "# Save the fully preprocessed data to a new file\n",
        "preprocessed_file_path_level3 = '/content/drive/My Drive/pre_telematics_syn_level3.csv'\n",
        "df_level3.to_csv(preprocessed_file_path_level3, index=False)\n",
        "print(df_level3.shape)  # Should print (100000, number of columns after feature engineering)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the response and feature columns\n",
        "response_cols = ['NB_Claim', 'AMT_Claim', 'ClaimYN']\n",
        "\n",
        "def split_data(df):\n",
        "    feature_cols = [col for col in df.columns if col not in response_cols]\n",
        "\n",
        "    # Split into 80% train and 20% test\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Further split train into train and validation\n",
        "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "    return train_df, val_df, test_df, feature_cols\n",
        "\n",
        "train_df_level1, val_df_level1, test_df_level1, feature_cols_level1 = split_data(df_level1)\n",
        "train_df_level2, val_df_level2, test_df_level2, feature_cols_level2 = split_data(df_level2)\n",
        "train_df_level3, val_df_level3, test_df_level3, feature_cols_level3 = split_data(df_level3)\n",
        "\n",
        "print(train_df_level1.shape, val_df_level1.shape, test_df_level1.shape)\n",
        "print(train_df_level2.shape, val_df_level2.shape, test_df_level2.shape)\n",
        "print(train_df_level3.shape, val_df_level3.shape, test_df_level3.shape)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjyELNmJ7EFQpAr9bLg2RK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}