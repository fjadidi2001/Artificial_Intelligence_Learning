


from google.colab import drive
drive.mount('/content/drive')


# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Specify file path
file_path = '/content/drive/My Drive/Dissertation/telematics_syn.csv'

# Import pandas (assuming you want to use it to read the CSV)
import pandas as pd

# Read the CSV file
df = pd.read_csv(file_path)

print(df.shape)  # Should print (100000, 52)

print(df.head())  # Example: print the first few rows of the dataframe









# Encoding categorical columns using one-hot encoding
categorical_cols = ['Marital', 'Insured.sex', 'Car.use', 'Region', 'Territory']
df = pd.get_dummies(df, columns=categorical_cols)

# Creating the ClaimYN variable
df['ClaimYN'] = df['NB_Claim'].apply(lambda x: 1 if x > 0 else 0)

# Saving the preprocessed data to a new file
preprocessed_file_path = '/content/drive/My Drive/pre_telematics_syn.csv'
df.to_csv(preprocessed_file_path, index=False)
print(df.shape)  # Should print (100000, number of columns after encoding)






from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Feature columns (excluding response columns)
feature_cols = [col for col in df.columns if col not in ['NB_Claim', 'AMT_Claim', 'ClaimYN']]

# Applying StandardScaler and MinMaxScaler
scaler = StandardScaler()
minmax_scaler = MinMaxScaler()

df[feature_cols] = scaler.fit_transform(df[feature_cols])
df[feature_cols] = minmax_scaler.fit_transform(df[feature_cols])

# Save the preprocessed data to a new file
preprocessed_file_path_scaled = '/content/drive/My Drive/pre_telematics_syn_scaled.csv'
df.to_csv(preprocessed_file_path_scaled, index=False)
print(df.shape)  # Should print (100000, number of columns after scaling and normalization)






# Advanced feature engineering steps, if any
# For example, aggregating harsh driving events:
# Check if the columns exist before using them
if all(col in df.columns for col in ['Accel.xx.miles', 'Brake.xx.miles', 'Left.turn.intensityxx', 'Right.turn.intensityxx']):
    df['HarshDriving'] = df['Accel.xx.miles'] + df['Brake.xx.miles'] + df['Left.turn.intensityxx'] + df['Right.turn.intensityxx']
else:
    print("Warning: One or more columns required for 'HarshDriving' calculation are missing.")

# Save the fully preprocessed data to a new file
preprocessed_file_path_final = '/content/drive/My Drive/pre_telematics_syn_advanced.csv'
df.to_csv(preprocessed_file_path_final, index=False)
print(df.shape)  # Should print (100000, number of columns after feature engineering)


from sklearn.model_selection import train_test_split

# Define the response and feature columns
response_cols = ['NB_Claim', 'AMT_Claim', 'ClaimYN']
feature_cols = [col for col in df.columns if col not in response_cols]

# Split into 80% train and 20% test
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Further split train into train and validation
train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)

print(train_df.shape, val_df.shape, test_df.shape)



!pip install pytorch-tabnet


# Install the library
!pip install pytorch-tabnet

# Import required libraries
from pytorch_tabnet.tab_model import TabNetClassifier
import numpy as np

# Prepare the data for TabNet
X_train = train_df[feature_cols].values
y_train = train_df['ClaimYN'].values
X_val = val_df[feature_cols].values
y_val = val_df['ClaimYN'].values
X_test = test_df[feature_cols].values
y_test = test_df['ClaimYN'].values

# Create and train the TabNet model
clf = TabNetClassifier()

clf.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_val, y_val)],
    eval_name=['train', 'val'],
    eval_metric=['accuracy'],
    max_epochs=50,
    patience=10,
    batch_size=1024,
    virtual_batch_size=128,
    num_workers=0,
    weights=1,
    drop_last=False
)

