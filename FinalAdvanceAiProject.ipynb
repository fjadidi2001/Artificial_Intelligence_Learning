{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkv7zBx+xHXPMvM4BgO4UC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Artificial_Intelligence_Learning/blob/master/FinalAdvanceAiProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QbGMaPOtCBT",
        "outputId": "43fb6921-89e6-4c80-f8b1-c1a05b531656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal path from node 0 to the closest destination: [0, 2, 3]\n",
            "Optimal path from node 1 to the closest destination: [1, 3]\n",
            "Optimal path from node 2 to the closest destination: [2, 3]\n",
            "Optimal path from node 3 to the closest destination: [3]\n",
            "Optimal path from node 4 to the closest destination: [4]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class QLearning:\n",
        "    def __init__(self, num_nodes, destinations, learning_rate=0.8, discount_factor=0.95, exploration_prob=0.2, epochs=1000):\n",
        "        self.num_nodes = num_nodes\n",
        "        self.destinations = destinations\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_prob = exploration_prob\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # Initialize Q matrix with zeros\n",
        "        self.Q = np.zeros((num_nodes, num_nodes))\n",
        "\n",
        "        # Initialize rewards matrix\n",
        "        self.R = np.zeros((num_nodes, num_nodes))\n",
        "\n",
        "    def add_edge(self, start_node, end_node, reward):\n",
        "        self.R[start_node][end_node] = reward\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            current_state = np.random.randint(0, self.num_nodes)\n",
        "\n",
        "            while current_state not in self.destinations:\n",
        "                possible_moves = np.where(self.R[current_state] > 0)[0]\n",
        "                if np.random.rand() < self.exploration_prob:\n",
        "                    action = np.random.choice(possible_moves)\n",
        "                else:\n",
        "                    action = np.argmax(self.Q[current_state])\n",
        "\n",
        "                next_state = action\n",
        "                reward = self.R[current_state][action]\n",
        "\n",
        "                self.Q[current_state][action] = reward + self.discount_factor * np.max(self.Q[next_state])\n",
        "\n",
        "                current_state = next_state\n",
        "\n",
        "    def find_optimal_path(self, start_node):\n",
        "        current_state = start_node\n",
        "        optimal_path = [current_state]\n",
        "\n",
        "        while current_state not in self.destinations:\n",
        "            action = np.argmax(self.Q[current_state])\n",
        "            current_state = action\n",
        "            optimal_path.append(current_state)\n",
        "\n",
        "        return optimal_path\n",
        "\n",
        "# Example usage\n",
        "num_nodes = 5\n",
        "destinations = [3, 4]\n",
        "q_learning = QLearning(num_nodes, destinations)\n",
        "\n",
        "# Add edges and rewards (adjust the rewards based on your specific graph)\n",
        "q_learning.add_edge(0, 1, 10)\n",
        "q_learning.add_edge(0, 2, 5)\n",
        "q_learning.add_edge(1, 3, 20)\n",
        "q_learning.add_edge(2, 3, 30)\n",
        "q_learning.add_edge(3, 4, 100)\n",
        "\n",
        "# Train the Q-learning model\n",
        "q_learning.train()\n",
        "\n",
        "# Find optimal path for each node to the closest destination\n",
        "for start_node in range(num_nodes):\n",
        "    optimal_path = q_learning.find_optimal_path(start_node)\n",
        "    print(f\"Optimal path from node {start_node} to the closest destination: {optimal_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CBiYkBgludZX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}