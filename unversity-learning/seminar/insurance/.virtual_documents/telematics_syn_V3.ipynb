





from google.colab import files

# Upload the file
uploaded = files.upload()


import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline





# Read the CSV file
df = pd.read_csv('telematics_syn.csv')


categorical_cols = ['Insured.sex', 'Marital', 'Car.use', 'Region', 'Territory']


preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), categorical_cols)
    ],
    remainder='passthrough'
)


numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns
numerical_cols = numerical_cols.drop(['NB_Claim', 'AMT_Claim'])  # Exclude target columns


preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(), categorical_cols)
    ])


pipeline = Pipeline(steps=[('preprocessor', preprocessor)])


X = df.drop(columns=['NB_Claim', 'AMT_Claim'])
X_preprocessed = pipeline.fit_transform(X)


y_nb_claim = df['NB_Claim']
y_amt_claim = df['AMT_Claim']


from sklearn.model_selection import train_test_split

# Assuming X_preprocessed is your preprocessed feature matrix and y_nb_claim and y_amt_claim are your target variables

# First, split the data into training and a temporary set (which will be further split into validation and test sets)
X_train, X_temp, y_nb_train, y_nb_temp, y_amt_train, y_amt_temp = train_test_split(
    X_preprocessed, y_nb_claim, y_amt_claim, test_size=0.3, random_state=42
)

# Now, split the temporary set into validation and test sets
X_val, X_test, y_nb_val, y_nb_test, y_amt_val, y_amt_test = train_test_split(
    X_temp, y_nb_temp, y_amt_temp, test_size=0.5, random_state=42
)

# Print the sizes of the resulting sets
print(f"Training set: {X_train.shape[0]} samples")
print(f"Validation set: {X_val.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")


import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Define the neural network model
def create_model(input_dim):
    model = Sequential([
        Dense(128, activation='relu', input_dim=input_dim),
        Dense(64, activation='relu'),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')  # Assuming binary classification for 'NB_Claim'
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Check data shapes and types
print(f"X_train shape: {X_train.shape}, y_nb_train shape: {y_nb_train.shape}")
print(f"X_val shape: {X_val.shape}, y_nb_val shape: {y_nb_val.shape}")
print(f"X_test shape: {X_test.shape}, y_nb_test shape: {y_nb_test.shape}")

# Train the model
input_dim = X_train.shape[1]
model = create_model(input_dim)

history = model.fit(
    X_train, y_nb_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_nb_val),
    verbose=1
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_nb_test, verbose=0)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Plot training & validation accuracy values
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.show()


!pip install pytorch_tabnet


import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from pytorch_tabnet.tab_model import TabNetClassifier
import matplotlib.pyplot as plt

# Ensure your data is in numpy array format
X_train_np = X_train.values if isinstance(X_train, pd.DataFrame) else X_train
X_val_np = X_val.values if isinstance(X_val, pd.DataFrame) else X_val
X_test_np = X_test.values if isinstance(X_test, pd.DataFrame) else X_test

y_nb_train_np = y_nb_train.values if isinstance(y_nb_train, pd.Series) else y_nb_train
y_nb_val_np = y_nb_val.values if isinstance(y_nb_val, pd.Series) else y_nb_val
y_nb_test_np = y_nb_test.values if isinstance(y_nb_test, pd.Series) else y_nb_test

# Initialize the TabNet model
tabnet_model = TabNetClassifier()

# Train the model
history = tabnet_model.fit(
    X_train=X_train_np, y_train=y_nb_train_np,
    eval_set=[(X_val_np, y_nb_val_np)],
    eval_metric=['accuracy'],
    max_epochs=100,
    patience=10,
    batch_size=1024,
    virtual_batch_size=128,
    num_workers=0,
    drop_last=False
)

# Evaluate the model on the test set
test_preds = tabnet_model.predict(X_test_np)
test_accuracy = accuracy_score(y_nb_test_np, test_preds)

# Extract training history
train_losses = history.history['loss']
val_accuracies = history.history['val_accuracy']

test_loss = train_losses[-1]  # Assuming the last epoch's loss is the test loss

print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Plot training & validation accuracy values
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(train_losses)
plt.title('Training Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')

plt.subplot(1, 2, 2)
plt.plot(val_accuracies)
plt.title('Validation Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')

plt.show()




