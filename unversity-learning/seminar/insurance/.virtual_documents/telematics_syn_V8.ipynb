


# Mount Google Drive
from google.colab import drive

drive.mount('/content/drive')

# Specify file path

file_path = '/content/drive/My Drive/telematics_syn.csv'

# Import pandas (assuming you want to use it to read the CSV)
import pandas as pd

# Read the CSV file
df = pd.read_csv(file_path)
print(df.shape)  # Should print (100000, 52)
print(df.head()) # To check the first few rows


# Adjust display options to show all columns
pd.set_option('display.max_columns', None)  # Show all columns
pd.set_option('display.expand_frame_repr', False)  # Prevent line wrapping

# Display the first few rows of the dataframe
print(df.head())


print("Summary statistics:\n", df.describe())


print("Missing values:\n", df.isnull().sum())


import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame
missing_values = df.isnull().sum()

# Plotting the missing values
missing_values.plot(kind='bar', figsize=(15, 6))
plt.title('Missing Values in Dataset')
plt.xlabel('Columns')
plt.ylabel('Number of Missing Values')
plt.show()


import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Numerical columns
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns

# Categorical columns
categorical_columns = ['Insured.sex', 'Marital', 'Car.use', 'Region']

# Subsample the data if necessary (e.g., 10% of the data)
sample_size = int(len(df) * 0.1)
df_sample = df.sample(n=sample_size, random_state=42)

# Plot histograms for numerical columns
for column in numerical_columns[:5]:  # Limit to first 5 numerical columns
    plt.figure(figsize=(10, 4))
    sns.histplot(df_sample[column], kde=True)
    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

# Plot box plots for numerical columns
for column in numerical_columns[:5]:  # Limit to first 5 numerical columns
    plt.figure(figsize=(10, 4))
    sns.boxplot(x=df_sample[column])
    plt.title(f'Box Plot of {column}')
    plt.xlabel(column)
    plt.show()

# Plot bar plots for categorical columns
for column in categorical_columns:
    plt.figure(figsize=(10, 4))
    sns.countplot(y=df_sample[column], order=df_sample[column].value_counts().index)
    plt.title(f'Bar Plot of {column}')
    plt.xlabel('Frequency')
    plt.ylabel(column)
    plt.show()

# Plot box plots for numerical columns grouped by categorical columns
for num_col in numerical_columns[:5]:  # Limit to first 5 numerical columns
    for cat_col in categorical_columns:
        plt.figure(figsize=(10, 4))
        sns.boxplot(x=df_sample[cat_col], y=df_sample[num_col])
        plt.title(f'Box Plot of {num_col} by {cat_col}')
        plt.xlabel(cat_col)
        plt.ylabel(num_col)
        plt.show()




import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Numerical columns
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns

# Select a subset of numerical columns for the heatmap (e.g., first 10 columns)
subset_columns = numerical_columns[:10]

# Compute the correlation matrix for the subset
correlation_matrix = df[subset_columns].corr()

# Plot the heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()


!pip install pytorch_tabnet


# Ensure necessary imports
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from pytorch_tabnet.tab_model import TabNetClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, matthews_corrcoef, log_loss, confusion_matrix, roc_curve
)
from sklearn.multiclass import OneVsRestClassifier
from imblearn.over_sampling import SMOTE



# Ignore warnings
warnings.filterwarnings('ignore')



# Assuming 'df' is your DataFrame and 'NB_Claim' is the target column
X = df.drop('NB_Claim', axis=1)
y = df['NB_Claim']


# Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object', 'category']).columns
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns

print(f"Categorical columns: {categorical_cols}")
print(f"Numerical columns: {numerical_cols}")

# Preprocessing pipelines for numerical and categorical data
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])



# Split the data into training and testing sets (80% train, 20% test)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)

# Split the temp set into validation and test sets (50% validation, 50% test from the temp set, i.e., 10% of the original dataset each)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Print the shapes of the resulting datasets
print(f"Training set shape: X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"Validation set shape: X_val: {X_val.shape}, y_val: {y_val.shape}")
print(f"Test set shape: X_test: {X_test.shape}, y_test: {y_test.shape}")



# Ensure necessary imports
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from pytorch_tabnet.tab_model import TabNetClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix
)
from sklearn.multiclass import OneVsRestClassifier

# Ignore warnings
warnings.filterwarnings('ignore')

# Load dataset
file_path = '/content/drive/My Drive/telematics_syn.csv'
data = pd.read_csv(file_path)

# Split data into features and target
X = data.drop('NB_Claim', axis=1)
y = data['NB_Claim']

# Get the number of unique classes
n_classes = len(np.unique(y))

# Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object', 'category']).columns
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns

# Preprocessing pipelines for numerical and categorical data
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Split the data
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Preprocess the training, validation, and test data
X_train = preprocessor.fit_transform(X_train)
X_val = preprocessor.transform(X_val)
X_test = preprocessor.transform(X_test)

# Balance the training dataset using SMOTE
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Number of epochs for TabNet
n_epochs = 10

# Define models
models = {
    'Logistic Regression': OneVsRestClassifier(LogisticRegression(max_iter=1000)),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', objective='multi:softprob', num_class=n_classes),
    'TabNet': TabNetClassifier(
        n_d=8,
        n_a=8,
        n_steps=3,
        gamma=1.5,
        lambda_sparse=1e-3,
        optimizer_fn=torch.optim.Adam,
        optimizer_params=dict(lr=2e-2),
        mask_type='entmax',
        scheduler_params={"step_size":10, "gamma":0.9},
        scheduler_fn=torch.optim.lr_scheduler.StepLR,
        verbose=1,
    )
}

def evaluate_model(model, X_train, X_val, X_test, y_train, y_val, y_test):
    """
    Evaluates a given model using various metrics.

    Args:
        model: The machine learning model to evaluate.
        X_train: Training data features.
        X_val: Validation data features.
        X_test: Test data features.
        y_train: Training data labels.
        y_val: Validation data labels.
        y_test: Test data labels.

    Returns:
        A dictionary containing the evaluation metrics.
    """

    # Handle models that use eval_set during training
    if 'eval_set' in model.fit.__code__.co_varnames:
        model.fit(X_train, y_train, eval_set=[(X_val, y_val)])
    else:
        model.fit(X_train, y_train)

    # Predict labels
    y_pred = model.predict(X_test)

    # Calculate evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    conf_matrix = confusion_matrix(y_test, y_pred)

    # Return a dictionary of metrics
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'conf_matrix': conf_matrix
    }

# Initial testing with raw data
results = {}
for name, model in models.items():
    print(f"Evaluating {name}...")
    if name == 'TabNet':
        # Pass n_epochs to the fit method for TabNet
        model.fit(
            X_train=X_train_res,
            y_train=y_train_res,
            eval_set=[(X_val, y_val)],
            max_epochs=n_epochs
        )
        metrics = evaluate_model(model, X_train_res, X_val, X_test, y_train_res, y_val, y_test)
        results[name] = metrics
    else:
        metrics = evaluate_model(model, X_train_res, X_val, X_test, y_train_res, y_val, y_test)
        results[name] = metrics

print("\nInitial Model Performance with Raw Data and Default Hyperparameters:")
for name, metrics in results.items():
    print(f"{name}:")
    for metric, value in metrics.items():
        if metric != 'conf_matrix':
            print(f"  {metric}: {value:.4f}")
    print()

# Function to plot confusion matrix
def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(10, 7))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {title}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

# When plotting confusion matrices
for name, metrics in results.items():
    plot_confusion_matrix(metrics['conf_matrix'], name)

# Combine all results into a final DataFrame for comparison
final_results = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [results[name]['accuracy'] for name in results],
    'Precision': [results[name]['precision'] for name in results],
    'Recall': [results[name]['recall'] for name in results],
    'F1 Score': [results[name]['f1'] for name in results]
})

print("Final Comparative Analysis Results:")
print(final_results)


# Define a list of hyperparameter combinations to try
hyperparameter_combinations = [
    {'n_d': 8, 'n_a': 8, 'n_steps': 3, 'gamma': 1.5, 'lambda_sparse': 1e-3, 'optimizer_params': {'lr': 2e-2}, 'scheduler_params': {"step_size": 10, "gamma": 0.9}},
    {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.3, 'lambda_sparse': 1e-2, 'optimizer_params': {'lr': 1e-2}, 'scheduler_params': {"step_size": 5, "gamma": 0.8}},
    # Add more combinations as needed
]

best_score = 0
best_model = None
best_params = {}

# Iterate through each hyperparameter combination
for params in hyperparameter_combinations:
    print(f"Trying hyperparameters: {params}")

    # Create a TabNet model with the current hyperparameters
    tabnet_model = TabNetClassifier(
        n_d=params['n_d'],
        n_a=params['n_a'],
        n_steps=params['n_steps'],
        gamma=params['gamma'],
        lambda_sparse=params['lambda_sparse'],
        optimizer_fn=torch.optim.Adam,
        optimizer_params=params['optimizer_params'],
        mask_type='entmax',
        scheduler_params=params['scheduler_params'],
        scheduler_fn=torch.optim.lr_scheduler.StepLR,
        verbose=1,
    )

    # Fit the model
    tabnet_model.fit(
        X_train=X_train_res,
        y_train=y_train_res,
        eval_set=[(X_val, y_val)],
        max_epochs=n_epochs
    )

    # Evaluate the model
    metrics = evaluate_model(tabnet_model, X_train_res, X_val, X_test, y_train_res, y_val, y_test)

    # Check if this model is the best so far
    if metrics['f1'] > best_score:
        best_score = metrics['f1']
        best_model = tabnet_model
        best_params = params
        results['TabNet'] = metrics

print("\nBest Hyperparameters:")
print(best_params)

print("\nInitial Model Performance with Raw Data and Default Hyperparameters:")
for name, metrics in results.items():
    print(f"{name}:")
    for metric, value in metrics.items():
        if metric != 'conf_matrix':
            print(f"  {metric}: {value:.4f}")
    print()

# Function to plot confusion matrix
def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(10, 7))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {title}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

# When plotting confusion matrices
for name, metrics in results.items():
    plot_confusion_matrix(metrics['conf_matrix'], name)

# Combine all results into a final DataFrame for comparison
final_results = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [results[name]['accuracy'] for name in results],
    'Precision': [results[name]['precision'] for name in results],
    'Recall': [results[name]['recall'] for name in results],
    'F1 Score': [results[name]['f1'] for name in results]
})

print("Final Comparative Analysis Results:")
print(final_results)


import matplotlib.pyplot as plt
import numpy as np

# Sample accuracy values for different models (replace with your actual data)
models = ['Logistic Regression', 'XGBoost', 'TabNet', 'Random Forest', 'LightGBM']
accuracies = [0.9681, 0.9952, 0.9897, 0.9959, 0.9953]

# Plotting
fig, ax = plt.subplots(figsize=(10, 6))

# Bar chart
bar_width = 0.35
index = np.arange(len(models))

bars = ax.bar(index, accuracies, bar_width, label='Accuracy', color=['blue', 'green', 'red', 'purple', 'orange'])

# Adding labels and title
ax.set_xlabel('Models')
ax.set_ylabel('Accuracy')
ax.set_title('Model Accuracy Comparison')
ax.set_xticks(index)
ax.set_xticklabels(models)
ax.legend()

# Adding accuracy values on top of the bars
for bar in bars:
    height = bar.get_height()
    ax.annotate(f'{height:.4f}', xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


from sklearn.ensemble import RandomForestClassifier

# Define the Random Forest model with class weight adjustment
rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)

# Fit the model
rf_model.fit(X_train_res, y_train_res)

# Evaluate the model
metrics = evaluate_model(rf_model, X_train_res, X_val, X_test, y_train_res, y_val, y_test)
results['Random Forest'] = metrics

print("\nInitial Model Performance with Raw Data and Default Hyperparameters:")
for name, metrics in results.items():
    print(f"{name}:")
    for metric, value in metrics.items():
        if metric != 'conf_matrix':
            print(f"  {metric}: {value:.4f}")
    print()

# Function to plot confusion matrix
def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(10, 7))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {title}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

# When plotting confusion matrices
for name, metrics in results.items():
    plot_confusion_matrix(metrics['conf_matrix'], name)

# Combine all results into a final DataFrame for comparison
final_results = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [results[name]['accuracy'] for name in results],
    'Precision': [results[name]['precision'] for name in results],
    'Recall': [results[name]['recall'] for name in results],
    'F1 Score': [results[name]['f1'] for name in results]
})

print("Final Comparative Analysis Results:")
print(final_results)


import lightgbm as lgb

# Define the LightGBM model with class weight adjustment
lgb_model = lgb.LGBMClassifier(n_estimators=100, class_weight='balanced', random_state=42)

# Fit the model with early stopping
lgb_model.fit(
    X_train_res, y_train_res,
    eval_set=[(X_val, y_val)],
    eval_metric='logloss',
    # early_stopping_rounds=10
)

# Evaluate the model
metrics = evaluate_model(lgb_model, X_train_res, X_val, X_test, y_train_res, y_val, y_test)
results['LightGBM'] = metrics

print("\nInitial Model Performance with Raw Data and Default Hyperparameters:")
for name, metrics in results.items():
    print(f"{name}:")
    for metric, value in metrics.items():
        if metric != 'conf_matrix':
            print(f"  {metric}: {value:.4f}")
    print()

# Function to plot confusion matrix
def plot_confusion_matrix(conf_matrix, title):
    plt.figure(figsize=(10, 7))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {title}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

# When plotting confusion matrices
for name, metrics in results.items():
    plot_confusion_matrix(metrics['conf_matrix'], name)

# Combine all results into a final DataFrame for comparison
final_results = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [results[name]['accuracy'] for name in results],
    'Precision': [results[name]['precision'] for name in results],
    'Recall': [results[name]['recall'] for name in results],
    'F1 Score': [results[name]['f1'] for name in results]
})

print("Final Comparative Analysis Results:")
print(final_results)
