





from google.colab import files

# Upload the file
uploaded = files.upload()





import pandas as pd

# Read the CSV file
df = pd.read_csv('telematics_syn.csv')

# Display the first few rows of the dataframe
df.head()





  print("Shape of the dataset:", df.shape)


# Adjust display options to show all columns
pd.set_option('display.max_columns', None)  # Show all columns
pd.set_option('display.expand_frame_repr', False)  # Prevent line wrapping

# Display the first few rows of the dataframe
print(df.head())





print("Summary statistics:\n", df.describe())





print("Missing values:\n", df.isnull().sum())








import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame
missing_values = df.isnull().sum()

# Plotting the missing values
missing_values.plot(kind='bar', figsize=(15, 6))
plt.title('Missing Values in Dataset')
plt.xlabel('Columns')
plt.ylabel('Number of Missing Values')
plt.show()





import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Numerical columns
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns

# Categorical columns
categorical_columns = ['Insured.sex', 'Marital', 'Car.use', 'Region']

# Subsample the data if necessary (e.g., 10% of the data)
sample_size = int(len(df) * 0.1)
df_sample = df.sample(n=sample_size, random_state=42)

# Plot histograms for numerical columns
for column in numerical_columns[:5]:  # Limit to first 5 numerical columns
    plt.figure(figsize=(10, 4))
    sns.histplot(df_sample[column], kde=True)
    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

# Plot box plots for numerical columns
for column in numerical_columns[:5]:  # Limit to first 5 numerical columns
    plt.figure(figsize=(10, 4))
    sns.boxplot(x=df_sample[column])
    plt.title(f'Box Plot of {column}')
    plt.xlabel(column)
    plt.show()

# Plot bar plots for categorical columns
for column in categorical_columns:
    plt.figure(figsize=(10, 4))
    sns.countplot(y=df_sample[column], order=df_sample[column].value_counts().index)
    plt.title(f'Bar Plot of {column}')
    plt.xlabel('Frequency')
    plt.ylabel(column)
    plt.show()

# Plot box plots for numerical columns grouped by categorical columns
for num_col in numerical_columns[:5]:  # Limit to first 5 numerical columns
    for cat_col in categorical_columns:
        plt.figure(figsize=(10, 4))
        sns.boxplot(x=df_sample[cat_col], y=df_sample[num_col])
        plt.title(f'Box Plot of {num_col} by {cat_col}')
        plt.xlabel(cat_col)
        plt.ylabel(num_col)
        plt.show()







import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Numerical columns
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns

# Select a subset of numerical columns for the heatmap (e.g., first 10 columns)
subset_columns = numerical_columns[:10]

# Compute the correlation matrix for the subset
correlation_matrix = df[subset_columns].corr()

# Plot the heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()





!pip install pytorch_tabnet





from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import xgboost as xgb
import torch
from pytorch_tabnet.tab_model import TabNetClassifier
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline








# First, split the dataset into training and temporary set (80% train, 20% temp)
train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)

# Then, split the temporary set into validation and test sets (50% val, 50% test)
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

# Save the splits to new CSV files
train_df.to_csv('train_set.csv', index=False)
val_df.to_csv('validation_set.csv', index=False)
test_df.to_csv('test_set.csv', index=False)

print("Training set size:", len(train_df))
print("Validation set size:", len(val_df))
print("Test set size:", len(test_df))





import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split

# Load the dataset
df = pd.read_csv('telematics_syn.csv')

# Simple Split: 80% train + validation, 20% test
train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Further split the train_val_df into training (75%) and validation (25%)
train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42)

# Identify feature types
numerical_features = [
    'Duration', 'Insured.age', 'Car.age', 'Credit.score',
    'Annual.miles.drive', 'Years.noclaims', 'Annual.pct.driven',
    'Total.miles.driven', 'Pct.drive.mon', 'Pct.drive.tue',
    'Pct.drive.wed', 'Pct.drive.thr', 'Pct.drive.fri',
    'Pct.drive.sat', 'Pct.drive.sun', 'Pct.drive.2hrs',
    'Pct.drive.3hrs', 'Pct.drive.4hrs', 'Pct.drive.wkday',
    'Pct.drive.wkend', 'Pct.drive.rush am', 'Pct.drive.rush pm',
    'Avgdays.week', 'Accel.06miles', 'Accel.08miles',
    'Accel.09miles', 'Accel.11miles', 'Accel.12miles',
    'Accel.14miles', 'Brake.06miles', 'Brake.08miles',
    'Brake.09miles', 'Brake.11miles', 'Brake.12miles',
    'Brake.14miles', 'Left.turn.intensity08',
    'Left.turn.intensity09', 'Left.turn.intensity10',
    'Left.turn.intensity11', 'Left.turn.intensity12',
    'Right.turn.intensity08', 'Right.turn.intensity09',
    'Right.turn.intensity10', 'Right.turn.intensity11',
    'Right.turn.intensity12', 'NB_Claim', 'AMT_Claim'
]

categorical_features = [
    'Insured.sex', 'Marital', 'Car.use', 'Region', 'Territory'
]

# Preprocessing pipeline
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# Fit and transform the training set
X_train = preprocessor.fit_transform(train_df.drop(columns=['AMT_Claim']))
y_train = train_df['AMT_Claim']

# Transform the validation set
X_val = preprocessor.transform(val_df.drop(columns=['AMT_Claim']))
y_val = val_df['AMT_Claim']

# Transform the test set
X_test = preprocessor.transform(test_df.drop(columns=['AMT_Claim']))
y_test = test_df['AMT_Claim']

print("Preprocessing for simple split complete.")
print("Training set shape:", X_train.shape)
print("Validation set shape:", X_val.shape)
print("Test set shape:", X_test.shape)






import pandas as pd
from sklearn.model_selection import train_test_split, KFold

# Read the CSV file
df = pd.read_csv('telematics_syn.csv')

# Simple Split: 80% train + validation, 20% test
train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Further split the train_val_df into training (80%) and validation (20%)
train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42)  # 0.25 of 0.8 is 0.2

# Save the simple splits to new CSV files
train_df.to_csv('train_set.csv', index=False)
val_df.to_csv('validation_set.csv', index=False)
test_df.to_csv('test_set.csv', index=False)

print("Simple Split sizes:")
print("Training set size:", len(train_df))
print("Validation set size:", len(val_df))
print("Test set size:", len(test_df))

# K-Fold Cross-Validation on the training set
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Iterate over the folds and split the training data
fold_number = 1
for train_index, val_index in kf.split(train_df):
    fold_train_df = train_df.iloc[train_index]
    fold_val_df = train_df.iloc[val_index]

    # Save the splits to new CSV files for each fold
    fold_train_df.to_csv(f'fold_train_set_{fold_number}.csv', index=False)
    fold_val_df.to_csv(f'fold_val_set_{fold_number}.csv', index=False)

    fold_number += 1

print("K-Fold splits saved for training set.")



from sklearn.model_selection import KFold

# Load the dataset (using the full dataset for K-Fold)
df = pd.read_csv('telematics_syn.csv')

# Identify feature types (same as before)
numerical_features = [
    'Duration', 'Insured.age', 'Car.age', 'Credit.score',
    'Annual.miles.drive', 'Years.noclaims', 'Annual.pct.driven',
    'Total.miles.driven', 'Pct.drive.mon', 'Pct.drive.tue',
    'Pct.drive.wed', 'Pct.drive.thr', 'Pct.drive.fri',
    'Pct.drive.sat', 'Pct.drive.sun', 'Pct.drive.2hrs',
    'Pct.drive.3hrs', 'Pct.drive.4hrs', 'Pct.drive.wkday',
    'Pct.drive.wkend', 'Pct.drive.rush am', 'Pct.drive.rush pm',
    'Avgdays.week', 'Accel.06miles', 'Accel.08miles',
    'Accel.09miles', 'Accel.11miles', 'Accel.12miles',
    'Accel.14miles', 'Brake.06miles', 'Brake.08miles',
    'Brake.09miles', 'Brake.11miles', 'Brake.12miles',
    'Brake.14miles', 'Left.turn.intensity08',
    'Left.turn.intensity09', 'Left.turn.intensity10',
    'Left.turn.intensity11', 'Left.turn.intensity12',
    'Right.turn.intensity08', 'Right.turn.intensity09',
    'Right.turn.intensity10', 'Right.turn.intensity11',
    'Right.turn.intensity12', 'NB_Claim', 'AMT_Claim'
]

categorical_features = [
    'Insured.sex', 'Marital', 'Car.use', 'Region', 'Territory'
]

# Preprocessing pipeline (same as before)
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# K-Fold Cross-Validation
kf = KFold(n_splits=3, shuffle=True, random_state=42)

for fold_number, (train_index, val_index) in enumerate(kf.split(df), 1):
    train_df = df.iloc[train_index]
    val_df = df.iloc[val_index]

    # Fit and transform the training set
    X_train = preprocessor.fit_transform(train_df.drop(columns=['AMT_Claim']))
    y_train = train_df['AMT_Claim']

    # Transform the validation set
    X_val = preprocessor.transform(val_df.drop(columns=['AMT_Claim']))
    y_val = val_df['AMT_Claim']

    print(f"Preprocessing for fold {fold_number} complete.")
    print("Training set shape:", X_train.shape)
    print("Validation set shape:", X_val.shape)






categorical_cols = ['Insured.sex', 'Marital', 'Car.use', 'Region', 'Territory']
numerical_cols = [col for col in df.columns if col not in categorical_cols and col not in ['NB_Claim', 'AMT_Claim']]


# Define the preprocessing for numerical data (standardization)
numerical_transformer = StandardScaler()

# Define the preprocessing for categorical data (one-hot encoding)
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])


# Create a pipeline with the preprocessor
pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

# Fit and transform the data
X_preprocessed = pipeline.fit_transform(df.drop(columns=['NB_Claim', 'AMT_Claim']))

# Convert the preprocessed data back to a DataFrame
# Note: The column names for the one-hot encoded features will be different
preprocessed_df = pd.DataFrame(X_preprocessed)


# Add the target columns back to the preprocessed DataFrame
preprocessed_df['NB_Claim'] = df['NB_Claim']
preprocessed_df['AMT_Claim'] = df['AMT_Claim']


preprocessed_df.to_csv('preprocessed_telematics_syn.csv', index=False)








from pytorch_tabnet.tab_model import TabNetClassifier
from sklearn.metrics import accuracy_score, log_loss

# Load the preprocessed data
preprocessed_df = pd.read_csv('preprocessed_telematics_syn.csv')
print(preprocessed_df.head())

# Define features and target
X = preprocessed_df.drop(columns=['NB_Claim', 'AMT_Claim'])
y = preprocessed_df['NB_Claim']  # Assuming NB_Claim is the target

# Create TabNet model
tabnet_model = TabNetClassifier(optimizer_fn=torch.optim.Adam,
                                 optimizer_params=dict(lr=2e-2),
                                 scheduler_fn=None,
                                 scheduler_params=None,
                                 mask_type='sparsemax')  # Use 'entmax' for better performance






import xgboost as xgb
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split

# Load the preprocessed data
preprocessed_df = pd.read_csv('preprocessed_telematics_syn.csv')

# Separate features and target variables
X = preprocessed_df.drop(columns=['NB_Claim', 'AMT_Claim'])
y = preprocessed_df['NB_Claim']  # Assuming you want to predict the number of claims

# Create the XGBoost model
model = xgb.XGBClassifier(n_estimators=5, learning_rate=0.1, max_depth=3, random_state=42)



# Split the data into training and validation sets (use the previous split)
train_df, val_df = train_test_split(preprocessed_df, test_size=0.2, random_state=42)

X_train = train_df.drop(columns=['NB_Claim', 'AMT_Claim'])
y_train = train_df['NB_Claim']
X_val = val_df.drop(columns=['NB_Claim', 'AMT_Claim'])
y_val = val_df['NB_Claim']

# Fit the model
model.fit(X_train, y_train)

# Predict on validation set
y_pred = model.predict(X_val)

# Calculate accuracy and loss
accuracy = accuracy_score(y_val, y_pred)
loss = mean_squared_error(y_val, y_pred)

print("Simple Split Accuracy:", accuracy)
print("Simple Split Loss:", loss)



from sklearn.model_selection import KFold
import matplotlib.pyplot as plt

kf = KFold(n_splits=3, shuffle=True, random_state=42)

accuracies = []
losses = []

for train_index, val_index in kf.split(X):
    X_train_kf, X_val_kf = X.iloc[train_index], X.iloc[val_index]
    y_train_kf, y_val_kf = y.iloc[train_index], y.iloc[val_index]

    # Fit the model
    model.fit(X_train_kf, y_train_kf)

    # Predict on validation set
    y_pred_kf = model.predict(X_val_kf)

    # Calculate accuracy and loss
    accuracy_kf = accuracy_score(y_val_kf, y_pred_kf)
    loss_kf = mean_squared_error(y_val_kf, y_pred_kf)

    accuracies.append(accuracy_kf)
    losses.append(loss_kf)

print("3-Fold Split Accuracies:", accuracies)
print("3-Fold Split Losses:", losses)


import pandas as pd
import xgboost as xgb
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split, KFold
import matplotlib.pyplot as plt

# Load the preprocessed data
preprocessed_df = pd.read_csv('preprocessed_telematics_syn.csv')

# Separate features and target variables
X = preprocessed_df.drop(columns=['NB_Claim', 'AMT_Claim'])
y = preprocessed_df['NB_Claim']

# Simple Split (80% train, 20% test)
train_df, test_df = train_test_split(preprocessed_df, test_size=0.2, random_state=42)
X_train = train_df.drop(columns=['NB_Claim', 'AMT_Claim'])
y_train = train_df['NB_Claim']
X_test = test_df.drop(columns=['NB_Claim', 'AMT_Claim'])
y_test = test_df['NB_Claim']

# Create the XGBoost model
model = xgb.XGBClassifier(n_estimators=5, learning_rate=0.1, max_depth=3, random_state=42)

# Fit the model on training data
model.fit(X_train, y_train)

# Predictions
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Calculate accuracy and loss for Simple Split
train_accuracy = accuracy_score(y_train, y_train_pred)
train_loss = mean_squared_error(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = mean_squared_error(y_test, y_test_pred)

# K-Fold Cross-Validation
kf = KFold(n_splits=3, shuffle=True, random_state=42)

kf_train_accuracies = []
kf_train_losses = []
kf_val_accuracies = []
kf_val_losses = []

for train_index, val_index in kf.split(X):
    X_train_kf, X_val_kf = X.iloc[train_index], X.iloc[val_index]
    y_train_kf, y_val_kf = y.iloc[train_index], y.iloc[val_index]

    # Fit the model
    model.fit(X_train_kf, y_train_kf)

    # Predictions
    y_train_kf_pred = model.predict(X_train_kf)
    y_val_kf_pred = model.predict(X_val_kf)

    # Calculate accuracy and loss for each fold
    kf_train_accuracies.append(accuracy_score(y_train_kf, y_train_kf_pred))
    kf_train_losses.append(mean_squared_error(y_train_kf, y_train_kf_pred))
    kf_val_accuracies.append(accuracy_score(y_val_kf, y_val_kf_pred))
    kf_val_losses.append(mean_squared_error(y_val_kf, y_val_kf_pred))

# Average K-Fold results
avg_kf_train_accuracy = sum(kf_train_accuracies) / len(kf_train_accuracies)
avg_kf_train_loss = sum(kf_train_losses) / len(kf_train_losses)
avg_kf_val_accuracy = sum(kf_val_accuracies) / len(kf_val_accuracies)
avg_kf_val_loss = sum(kf_val_losses) / len(kf_val_losses)

# Plotting
labels = ['Train (Simple Split)', 'Test (Simple Split)', 'Train (K-Fold)', 'Validation (K-Fold)']
accuracies = [train_accuracy, test_accuracy, avg_kf_train_accuracy, avg_kf_val_accuracy]
losses = [train_loss, test_loss, avg_kf_train_loss, avg_kf_val_loss]

x = range(len(labels))

plt.figure(figsize=(14, 6))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.bar(x, accuracies, color='blue', alpha=0.7)
plt.xticks(x, labels)
plt.title('Accuracy')
plt.ylim(0, 1)

# Loss plot
plt.subplot(1, 2, 2)
plt.bar(x, losses, color='red', alpha=0.7)
plt.xticks(x, labels)
plt.title('Loss')
plt.ylim(0, max(losses) + 1)

plt.tight_layout()
plt.show()



print(y.value_counts())



from sklearn.metrics import classification_report

print("Classification Report for Test Set:")
print(classification_report(y_test, y_test_pred))



from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)



model = xgb.XGBClassifier(n_estimators=50, learning_rate=0.1, max_depth=3,
                           scale_pos_weight=95728/4061, random_state=42)



import pandas as pd
import xgboost as xgb
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

# Load the preprocessed data
preprocessed_df = pd.read_csv('preprocessed_telematics_syn.csv')

# Separate features and target variables
X = preprocessed_df.drop(columns=['NB_Claim', 'AMT_Claim'])
y = preprocessed_df['NB_Claim']

# Simple Split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Create the XGBoost model with class weight adjustment
model = xgb.XGBClassifier(n_estimators=50, learning_rate=0.1, max_depth=3, random_state=42)

# Fit the model on resampled training data
model.fit(X_resampled, y_resampled)

# Predictions on the test set
y_test_pred = model.predict(X_test)

# Classification report
print("Classification Report for Test Set:")
print(classification_report(y_test, y_test_pred))






import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, log_loss
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Step 1: Load the preprocessed dataset
df = pd.read_csv('preprocessed_telematics_syn.csv')

# Define features and target variable
X = df.drop(columns=['NB_Claim', 'AMT_Claim'])
y = df['NB_Claim']  # Assuming you want to predict the number of claims

# Step 2: Evaluate model with simple split
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a logistic regression model
model = LogisticRegression(max_iter=1000)

# Train the model
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)

# Calculate accuracy and log loss
accuracy_simple = accuracy_score(y_test, y_pred)
log_loss_simple = log_loss(y_test, y_pred_proba)

print("Simple Split Evaluation:")
print("Accuracy:", accuracy_simple)
print("Log Loss:", log_loss_simple)

# Step 3: Evaluate model with 3-fold split
kf = KFold(n_splits=3, shuffle=True, random_state=42)

accuracies = []
log_losses = []

# Perform K-Fold cross-validation
for train_index, val_index in kf.split(X):
    X_train_kf, X_val_kf = X.iloc[train_index], X.iloc[val_index]
    y_train_kf, y_val_kf = y.iloc[train_index], y.iloc[val_index]

    # Train the model
    model.fit(X_train_kf, y_train_kf)

    # Predict on the validation set
    y_val_pred = model.predict(X_val_kf)
    y_val_pred_proba = model.predict_proba(X_val_kf)

    # Calculate accuracy and log loss
    accuracy_kf = accuracy_score(y_val_kf, y_val_pred)
    log_loss_kf = log_loss(y_val_kf, y_val_pred_proba)

    accuracies.append(accuracy_kf)
    log_losses.append(log_loss_kf)

print("3-Fold Split Evaluation:")
print("Mean Accuracy:", np.mean(accuracies))
print("Mean Log Loss:", np.mean(log_losses))

# Plotting accuracy and loss
plt.figure(figsize=(12, 5))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(range(1, 4), accuracies, marker='o', label='Accuracy')
plt.title('Model Accuracy (3-Fold)')
plt.xlabel('Fold Number')
plt.ylabel('Accuracy')
plt.xticks(range(1, 4))
plt.grid()
plt.legend()

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(range(1, 4), log_losses, marker='o', color='r', label='Log Loss')
plt.title('Model Log Loss (3-Fold)')
plt.xlabel('Fold Number')
plt.ylabel('Log Loss')
plt.xticks(range(1, 4))
plt.grid()
plt.legend()

plt.tight_layout()
plt.show()




